{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Python 2 vs 3\n",
    "## Conda\n",
    "## Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing VCF - adding readcounts using bam-readcount and VAtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this module we'll be working with a somatic exome VCF file created by the Mutect variant caller with some basic filtering already done. This VCF can be found in the `week_09` folder of the [bfx-workshop repository](https://github.com/genome/bfx-workshop). Let's create a working directory and download this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/john/mgi_workshop\n",
      "--2020-11-13 10:21:19--  https://github.com/genome/bfx-workshop/blob/master/week_09/mutect.filtered.vcf.gz?raw=true\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/genome/bfx-workshop/raw/master/week_09/mutect.filtered.vcf.gz [following]\n",
      "--2020-11-13 10:21:19--  https://github.com/genome/bfx-workshop/raw/master/week_09/mutect.filtered.vcf.gz\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/genome/bfx-workshop/master/week_09/mutect.filtered.vcf.gz [following]\n",
      "--2020-11-13 10:21:19--  https://raw.githubusercontent.com/genome/bfx-workshop/master/week_09/mutect.filtered.vcf.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.68.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.68.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 96958 (95K) [application/octet-stream]\n",
      "Saving to: ‘/home/john/mgi_workshop/bfx_workshop_week_09/mutect.filtered.vcf.gz’\n",
      "\n",
      "/home/john/mgi_work 100%[===================>]  94.69K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2020-11-13 10:21:20 (23.6 MB/s) - ‘/home/john/mgi_workshop/bfx_workshop_week_09/mutect.filtered.vcf.gz’ saved [96958/96958]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!echo $PWD\n",
    "!mkdir -p $PWD/bfx_workshop_week_09\n",
    "!wget https://github.com/genome/bfx-workshop/blob/master/week_09/mutect.filtered.vcf.gz?raw=true -O $PWD/bfx_workshop_week_09/mutect.filtered.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need the reference and tumor bam files used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://analysis-workflows-example-data/somatic_inputs/hla_and_brca_genes.fa...\n",
      "- [1/1 files][246.3 MiB/246.3 MiB] 100% Done                                    \n",
      "Operation completed over 1 objects/246.3 MiB.                                    \n",
      "Copying gs://analysis-workflows-example-data/somatic_inputs/hla_and_brca_genes.fa.fai...\n",
      "/ [1/1 files][   54.0 B/   54.0 B] 100% Done                                    \n",
      "Operation completed over 1 objects/54.0 B.                                       \n",
      "--2020-11-13 10:26:28--  https://xfer.genome.wustl.edu/gxfer1/project/cancer-genomics/bfx_workshop/tumor.bam\n",
      "Resolving xfer.genome.wustl.edu (xfer.genome.wustl.edu)... 128.252.233.42\n",
      "Connecting to xfer.genome.wustl.edu (xfer.genome.wustl.edu)|128.252.233.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 128053546 (122M)\n",
      "Saving to: ‘/home/john/mgi_workshop/bfx_workshop_week_09/tumor.bam’\n",
      "\n",
      "/home/john/mgi_work 100%[===================>] 122.12M  87.3MB/s    in 1.4s    \n",
      "\n",
      "2020-11-13 10:26:30 (87.3 MB/s) - ‘/home/john/mgi_workshop/bfx_workshop_week_09/tumor.bam’ saved [128053546/128053546]\n",
      "\n",
      "--2020-11-13 10:26:30--  https://xfer.genome.wustl.edu/gxfer1/project/cancer-genomics/bfx_workshop/tumor.bam.bai\n",
      "Resolving xfer.genome.wustl.edu (xfer.genome.wustl.edu)... 128.252.233.42\n",
      "Connecting to xfer.genome.wustl.edu (xfer.genome.wustl.edu)|128.252.233.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2208712 (2.1M)\n",
      "Saving to: ‘/home/john/mgi_workshop/bfx_workshop_week_09/tumor.bam.bai’\n",
      "\n",
      "/home/john/mgi_work 100%[===================>]   2.11M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-11-13 10:26:30 (18.1 MB/s) - ‘/home/john/mgi_workshop/bfx_workshop_week_09/tumor.bam.bai’ saved [2208712/2208712]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!docker run -v $PWD/bfx_workshop_week_09:/staging mgibio/data_downloader:0.1.0 gsutil -m cp gs://analysis-workflows-example-data/somatic_inputs/hla_and_brca_genes.fa /staging\n",
    "!docker run -v $PWD/bfx_workshop_week_09:/staging mgibio/data_downloader:0.1.0 gsutil -m cp gs://analysis-workflows-example-data/somatic_inputs/hla_and_brca_genes.fa.fai /staging\n",
    "!wget https://xfer.genome.wustl.edu/gxfer1/project/cancer-genomics/bfx_workshop/tumor.bam -O $PWD/bfx_workshop_week_09/tumor.bam\n",
    "!wget https://xfer.genome.wustl.edu/gxfer1/project/cancer-genomics/bfx_workshop/tumor.bam.bai -O $PWD/bfx_workshop_week_09/tumor.bam.bai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting multialleleic sites using vt decompose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our VCF might contain variants with multiple alt alleles. In these cases the ALT field of the VCF will have multiple alt alleles in it. Take for example this variant:\n",
    "```\n",
    "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tExome_Normal\tExome_Tumor\n",
    "chr17\t3017916\t.\tCGTGT\tC,CGT\t.\tgermline;multiallelic;normal_artifact\tAS_FilterStatus=weak_evidence|SITE;AS_SB_TABLE=31,0|3,0|9,0;DP=48;ECNT=1;GERMQ=1;MBQ=30,30,30;MFRL=0,0,0;MMQ=60,60,60;MPOS=49,31;NALOD=0.710,-7.297e+00;NLOD=2.65,-6.178e+00;POPAF=6.00,6.00;RPA=16,14,15;RU=GT;STR;STRQ=93;TLOD=6.76,12.45\tGT:AD:AF:DP:F1R2:F2R1:SB\t0/0:9,0,3:0.066,0.271:12:0,0,0:8,0,3:9,0,3,0\t0/1/2:22,3,6:0.117,0.205:31:0,0,0:22,3,6:22,0,9,0\n",
    "```\n",
    "This might happen if both chromsomes have a mutation at the same position, but the exact mutation differs between the two chromosomes. It might also happen if there is a subclonal mutation in some tumor cells. It might also just be an artifact.\n",
    "\n",
    "It is usually easier to process a VCF if these sort of variants are preprocessed to split up multi-allelic sites since some information is encoded on a per-allele basis (e.g., per-allele depth, per-allele VAF). \n",
    "\n",
    "vt decompose is part of the [vt tool package](https://genome.sph.umich.edu/wiki/Vt) and available on quay container at `quay.io/biocontainers/vt:0.57721--hf74b74d_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decompose v0.5\n",
      "\n",
      "options:     input VCF file        /data/mutect.filtered.vcf.gz\n",
      "         [s] smart decomposition   true (experimental)\n",
      "         [o] output VCF file       /data/mutect.filtered.decomposed.vcf.gz\n",
      "\n",
      "\n",
      "stats: no. variants                 : 768\n",
      "       no. biallelic variants       : 747\n",
      "       no. multiallelic variants    : 21\n",
      "\n",
      "       no. additional biallelics    : 24\n",
      "       total no. of biallelics      : 792\n",
      "\n",
      "Time elapsed: 0.04s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!docker run -v $PWD/bfx_workshop_week_09:/data -it quay.io/biocontainers/vt:0.57721--hf74b74d_1 vt decompose /data/mutect.filtered.vcf.gz -s -o /data/mutect.filtered.decomposed.vcf.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running vt decompose the above variant is split up into two lines and looks like this:\n",
    "```\n",
    "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tExome_Normal\tExome_Tumor\n",
    "chr17\t3017916\t.\tCGTGT\tC\t.\tgermline;multiallelic;normal_artifact\tAS_FilterStatus=weak_evidence|SITE;AS_SB_TABLE=31,0|3,0|9,0;DP=48;ECNT=1;GERMQ=1;MBQ=30,30;MFRL=0,0;MMQ=60,60;MPOS=49;NALOD=0.71;NLOD=2.65;POPAF=6;RPA=16,14;RU=GT;STR;STRQ=93;TLOD=6.76;OLD_MULTIALLELIC=chr17:3017916:CGTGT/C/CGT\tGT:AD:AF:DP:F1R2:F2R1:SB\t0/0:9,0:0.066:12:0,0:8,0:9,0,3,0\t0/1/.:22,3:0.117:31:0,0:22,3:22,0,9,0\n",
    "chr17\t3017916\t.\tCGTGT\tCGT\t.\tgermline;multiallelic;normal_artifact\tAS_FilterStatus=weak_evidence|SITE;AS_SB_TABLE=31,0|3,0|9,0;DP=48;ECNT=1;GERMQ=1;MBQ=30,30;MFRL=0,0;MMQ=60,60;MPOS=31;NALOD=-7.297;NLOD=-6.178;POPAF=6;RPA=16,15;RU=GT;STR;STRQ=93;TLOD=12.45;OLD_MULTIALLELIC=chr17:3017916:CGTGT/C/CGT\tGT:AD:AF:DP:F1R2:F2R1:SB\t0/0:9,3:0.271:12:0,0:8,3:9,0,3,0\t0/./1:22,6:0.205:31:0,0:22,6:22,0,9,0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bam-readcount\n",
    "Some variant callers will already output read depth and allelic depths but this is useful in cases where this information is not already present in the VCF. This is also useful if you run RNAseq on top of somatic variant calling to add RNA coverage information to your VCF.\n",
    "\n",
    "We will be using the `mgibio/bam_readcount_helper-cwl:1.1.1` docker container to run bam-readcount. This Docker image already has bam-readcount installed and it also contains a script that will take care of creating a region list from your VCF, which is a required input to bam-readcount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required inputs\n",
    "- vcf\n",
    "- sample name\n",
    "- reference fasta\n",
    "- bam file\n",
    "- output file prefix\n",
    "- output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex variant or MNP will be skipped: chr17\t3017916\tCGTGT\tCGT\r\n",
      "Complex variant or MNP will be skipped: chr17\t7578700\tCA\tCAA\r\n",
      "Complex variant or MNP will be skipped: chr17\t7578700\tCA\tCAAA\r\n",
      "Complex variant or MNP will be skipped: chr17\t8513688\tGTT\tGT\r\n",
      "Complex variant or MNP will be skipped: chr17\t17249049\tGAA\tGA\r\n",
      "Complex variant or MNP will be skipped: chr17\t32939409\tGAA\tGA\r\n",
      "Complex variant or MNP will be skipped: chr17\t39204770\tGT\tGTT\r\n",
      "Complex variant or MNP will be skipped: chr17\t42728741\tCA\tCAA\r\n",
      "Complex variant or MNP will be skipped: chr17\t48916258\tGT\tTT\r\n",
      "Complex variant or MNP will be skipped: chr17\t50964587\tCA\tCAA\r\n",
      "Complex variant or MNP will be skipped: chr17\t56321063\tGAA\tGA\r\n",
      "Complex variant or MNP will be skipped: chr17\t56321063\tGAA\tGAAA\r\n",
      "Complex variant or MNP will be skipped: chr17\t67077719\tCA\tCAA\r\n",
      "Complex variant or MNP will be skipped: chr17\t67909448\tCT\tCTT\r\n",
      "Complex variant or MNP will be skipped: chr17\t68045755\tATT\tAT\r\n",
      "Complex variant or MNP will be skipped: chr17\t69520199\tGT\tGTT\r\n",
      "Complex variant or MNP will be skipped: chr17\t75319447\tCT\tCTT\r\n",
      "Complex variant or MNP will be skipped: chr17\t75471713\tCA\tCAA\r\n",
      "Complex variant or MNP will be skipped: chr17\t76227117\tGT\tGTT\r\n",
      "Complex variant or MNP will be skipped: chr17\t76480007\tATGTG\tATG\r\n",
      "Complex variant or MNP will be skipped: chr17\t76740917\tCTT\tCT\r\n",
      "Complex variant or MNP will be skipped: chr17\t76740917\tCTT\tCTTT\r\n",
      "Complex variant or MNP will be skipped: chr17\t81887958\tTA\tTAA\r\n"
     ]
    }
   ],
   "source": [
    "!docker run -v $PWD/bfx_workshop_week_09:/data -it mgibio/bam_readcount_helper-cwl:1.1.1 python /usr/bin/bam_readcount_helper.py /data/mutect.filtered.decomposed.vcf.gz Exome_Tumor /data/hla_and_brca_genes.fa /data/tumor.bam Exome_Tumor /data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAtools\n",
    "[VAtools](http://www.vatools.org) is a python package that provides a suite of tools that help with processing VCF annotations. We will be using the [vcf-readcount-annotator tool](https://vatools.readthedocs.io/en/latest/vcf_readcount_annotator.html) included with VAtools to write the readcounts calculated in the previous step to our VCF. VAtools is available as a Docker image at `griffithlab/vatools:4.1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -v $PWD/bfx_workshop_week_09:/data -it griffithlab/vatools:4.1.0 vcf-readcount-annotator /data/mutect.filtered.decomposed.vcf.gz /data/Exome_Tumor_Exome_Tumor_bam_readcount_snv.tsv DNA -t snv -s Exome_Tumor -o /data/mutect.filtered.decomposed.readcount_snvs.vcf.gz\n",
    "!docker run -v $PWD/bfx_workshop_week_09:/data -it griffithlab/vatools:4.1.0 vcf-readcount-annotator /data/mutect.filtered.decomposed.readcount_snvs.vcf.gz /data/Exome_Tumor_Exome_Tumor_bam_readcount_indel.tsv DNA -s Exome_Tumor -t indel -o /data/mutect.filtered.decomposed.readcount_snvs_indel.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing VCFs in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyVCF vs VCFPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyVCF](https://pyvcf.readthedocs.io/en/latest/) is the \"original\" Python VCF parser. It does a good job reading VCFs but doesn't support modifying VCF entries very well. It also doesn't appear to be maintained anymore. [VCFPy](https://vcfpy.readthedocs.io/en/stable/) was created to solve this problem. For that reason we'll be using VCFPy for the next tasks.\n",
    "\n",
    "First, we need to ensure that the `vcfpy` package is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vcfpy in /home/john/miniconda3/lib/python3.8/site-packages (0.13.3)\n",
      "Requirement already satisfied: pysam in /home/john/miniconda3/lib/python3.8/site-packages (0.16.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vcfpy pysam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in a VCF and exploring its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vcfpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the VCF reader object from your VCF path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_reader = vcfpy.Reader.from_path(\"bfx_workshop_week_09/mutect.filtered.decomposed.readcount_snvs_indel.vcf.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which samples are in your VCF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Exome_Normal', 'Exome_Tumor']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcf_reader.header.samples.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which FILTERS are defined in the VCF header?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PASS',\n",
       " 'FAIL',\n",
       " 'base_qual',\n",
       " 'clustered_events',\n",
       " 'contamination',\n",
       " 'duplicate',\n",
       " 'fragment',\n",
       " 'germline',\n",
       " 'haplotype',\n",
       " 'low_allele_frac',\n",
       " 'map_qual',\n",
       " 'multiallelic',\n",
       " 'n_ratio',\n",
       " 'normal_artifact',\n",
       " 'orientation',\n",
       " 'panel_of_normals',\n",
       " 'position',\n",
       " 'possible_numt',\n",
       " 'slippage',\n",
       " 'strand_bias',\n",
       " 'strict_strand',\n",
       " 'weak_evidence']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcf_reader.header.filter_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar methods `info_ids` and `format_ids` exist for the INFO and FORMAT fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information for a specific INFO header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InfoHeaderLine('INFO', '<ID=DP,Number=1,Type=Integer,Description=\"Approximate read depth; some reads may have been filtered\">', {'ID': 'DP', 'Number': 1, 'Type': 'Integer', 'Description': 'Approximate read depth; some reads may have been filtered'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcf_reader.header.get_info_field_info('DP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information for each variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in vcf_reader:\n",
    "    #Get all FILTER fields applied to this variant \n",
    "    entry.FILTER\n",
    "    #Get the VAFs of a variant\n",
    "    calls = entry.call_for_sample['Exome_Tumor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After your're done with all processing, you will need to close the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering a VCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a filtered VCF so that only variants with a `PASS` filter and a VAF over 0.25 will remain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vcfpy\n",
    "vcf_reader = vcfpy.Reader.from_path(\"bfx_workshop_week_09/mutect.filtered.decomposed.vcf.gz\")\n",
    "vcf_writer = vcfpy.Writer.from_path(\"bfx_workshop_week_09/mutect.filtered.decomposed.pass_vaf_filtered.vcf.gz\", vcf_reader.header)\n",
    "for entry in vcf_reader:\n",
    "    if 'PASS' in entry.FILTER and entry.call_for_sample['Exome_Tumor'].data['AF'][0] > 0.25:\n",
    "        vcf_writer.write_record(entry)\n",
    "vcf_reader.close()\n",
    "vcf_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a human-readable TSV file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "VCFs can often be hard to read since a lot of information is presented in a condensed manner. Let's output some of its information in an easier understandable TSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vcfpy\n",
    "import csv\n",
    "\n",
    "vcf_reader = vcfpy.Reader.from_path(\"bfx_workshop_week_09/mutect.filtered.decomposed.vcf.gz\")\n",
    "with open(\"bfx_workshop_week_09/out.csv\", 'w') as out_fh:\n",
    "    headers = ['CHROM', 'POS', 'REF', 'ALT', 'FILTER', 'DEPTH', 'VAF']\n",
    "    tsv_writer = csv.DictWriter(out_fh, delimiter = '\\t', fieldnames = headers)\n",
    "    tsv_writer.writeheader()\n",
    "    for entry in vcf_reader:\n",
    "        out = {\n",
    "            'CHROM': entry.CHROM,\n",
    "            'POS': entry.POS,\n",
    "            'REF': entry.REF,\n",
    "            'ALT': entry.ALT,\n",
    "            'FILTER': ','.join(entry.FILTER),\n",
    "            'DEPTH': entry.call_for_sample['Exome_Tumor'].data['DP'],\n",
    "            'VAF': ','.join( [str(vaf) for vaf in entry.call_for_sample['Exome_Tumor'].data['AF']] )\n",
    "        }\n",
    "        tsv_writer.writerow(out)\n",
    "vcf_reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "samtools pysam \n",
    "biotools? biopython for fasta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
